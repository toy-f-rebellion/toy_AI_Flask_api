{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "358zkRZEdUvC",
        "outputId": "97bf4dce-4312-4a0f-9c67-c06595f18d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'toy_ai'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 83 (delta 24), reused 14 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (83/83), 2.18 MiB | 4.71 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n",
            "/content/toy_ai\n",
            "Cloning into 'Modified-KoBERT'...\n",
            "remote: Enumerating objects: 449, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 449 (delta 138), reused 120 (delta 110), pack-reused 273\u001b[K\n",
            "Receiving objects: 100% (449/449), 227.95 KiB | 5.43 MiB/s, done.\n",
            "Resolving deltas: 100% (226/226), done.\n",
            "Collecting kobert_tokenizer (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-s0h_orqx/kobert-tokenizer_7cce2fb15de24231b90afbb7fa47198b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-s0h_orqx/kobert-tokenizer_7cce2fb15de24231b90afbb7fa47198b\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18 (from -r requirements.txt (line 1))\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonnlp==0.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading gluonnlp-0.8.0.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mxnet (from -r requirements.txt (line 3))\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime==1.12.0 (from -r requirements.txt (line 4))\n",
            "  Downloading onnxruntime-1.12.0-cp310-cp310-manylinux_2_27_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.96 (from -r requirements.txt (line 5))\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.0.1+cu118)\n",
            "Collecting transformers (from -r requirements.txt (line 7))\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.66.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0->-r requirements.txt (line 2)) (1.23.5)\n",
            "Collecting coloredlogs (from onnxruntime==1.12.0->-r requirements.txt (line 4))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (4.7.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (16.0.6)\n",
            "Collecting botocore<1.19.0,>=1.18.18 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet->-r requirements.txt (line 3)) (2.31.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet->-r requirements.txt (line 3))\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 9)) (2023.3)\n",
            "Collecting urllib3<1.26,>=1.20 (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 7)) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (2023.7.22)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.12.0->-r requirements.txt (line 4))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.12.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Building wheels for collected packages: gluonnlp, kobert_tokenizer\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.8.0-py3-none-any.whl size=292696 sha256=efdbfcdf478ca8302799ca8b7c7b728be04569ae53dc07c1661433d2852b198e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/cc/dc/7ec84dced25f738b8be400101abb67e4b50c905090a51017e4\n",
            "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=d9291142176995ce84db9bbda44b81258b87cb529b84589256cf1df1e1cc953c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qr_b1z89/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n",
            "Successfully built gluonnlp kobert_tokenizer\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, kobert_tokenizer, urllib3, jmespath, humanfriendly, graphviz, gluonnlp, coloredlogs, botocore, s3transfer, onnxruntime, mxnet, huggingface-hub, transformers, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed boto3-1.15.18 botocore-1.18.18 coloredlogs-15.0.1 gluonnlp-0.8.0 graphviz-0.8.4 huggingface-hub-0.16.4 humanfriendly-10.0 jmespath-0.10.0 kobert_tokenizer-0.1 mxnet-1.9.1 onnxruntime-1.12.0 s3transfer-0.3.7 safetensors-0.3.3 sentencepiece-0.1.96 tokenizers-0.13.3 transformers-4.32.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/toy-f-rebellion/toy_ai.git\n",
        "%cd /content/toy_ai\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNjzjyZLVU07"
      },
      "source": [
        "# 1. Environment Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mWE5F3FVU08"
      },
      "source": [
        "## (1) 라이브러리 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9n1-B_V8VU09"
      },
      "outputs": [],
      "source": [
        "# Setting Library\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQW0ZxLdVU0-"
      },
      "source": [
        "## (2). 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jCKFbl08VU0-"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/toy_ai/Dataset/5차년도_2차.csv', encoding = 'CP949')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXNbwJZ1VU0-"
      },
      "source": [
        "# 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT2D8mQsVU0-"
      },
      "source": [
        "## (1) Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVYETVzDbec2"
      },
      "source": [
        "- 라벨값 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKcIqT3QVU0-",
        "outputId": "d3ae2f19-324f-436a-ed18-ddfea5a23806"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['happiness', 'neutral', 'sadness', 'angry', 'surprise', 'disgust',\n",
              "       'fear'], dtype=object)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['상황'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TMRPLNUbheJ"
      },
      "source": [
        "- 라벨링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3v2F5cgJbjb7"
      },
      "outputs": [],
      "source": [
        "labels = list(data['상황'].unique())\n",
        "for i in range(len(labels)) :\n",
        "  data.loc[data['상황'] == labels[i], '상황'] = i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mbXi44RVU0_"
      },
      "source": [
        "## (2) KoBERT 모델의 입력 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RyFeOJhtVU0_"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "for text, label in zip(data['발화문'], data['상황'])  :\n",
        "    data = []\n",
        "    data.append(text)\n",
        "    data.append(str(label))\n",
        "    dataset.append(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5jGWiPfVU0_"
      },
      "source": [
        "## (3) Train/Test Dataset Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sy7vKhhzVU0_"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(dataset, test_size = 0.2,random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYtuvlsleT9z"
      },
      "source": [
        "# 3. 데이터 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xrqOd57GeKD4"
      },
      "outputs": [],
      "source": [
        "# train_data를 파일에 저장\n",
        "with open('/content/toy_ai/train_data.pkl', 'wb') as f:\n",
        "    pickle.dump(train_data, f)\n",
        "# test_data를 파일에 저장\n",
        "with open('/content/toy_ai/test_data.pkl', 'wb') as f:\n",
        "    pickle.dump(test_data, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
